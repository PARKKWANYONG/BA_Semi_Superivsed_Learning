{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb71893",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-27T08:24:53.027Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/27/2022 17:24:53 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "12/27/2022 17:24:53 - INFO - __main__ -   {'gpu_id': 0, 'num_workers': 4, 'dataset': 'cifar10', 'num_labeled': 4000, 'expand_labels': False, 'arch': 'wideresnet', 'total_steps': 1048576, 'eval_step': 1024, 'start_epoch': 0, 'batch_size': 64, 'lr': 0.03, 'warmup': 0, 'wdecay': 0.0005, 'nesterov': True, 'use_ema': True, 'ema_decay': 0.999, 'mu': 7, 'lambda_u': 1, 'T': 1, 'threshold': 0.95, 'out': 'result', 'resume': '', 'seed': None, 'amp': False, 'opt_level': 'O1', 'local_rank': -1, 'no_progress': False, 'world_size': 1, 'n_gpu': 0, 'device': device(type='cpu')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/27/2022 17:24:54 - INFO - models.wideresnet -   Model: WideResNet 28x2\n",
      "12/27/2022 17:24:54 - INFO - __main__ -   Total params: 1.47M\n",
      "12/27/2022 17:24:54 - INFO - __main__ -   ***** Running training *****\n",
      "12/27/2022 17:24:54 - INFO - __main__ -     Task = cifar10@4000\n",
      "12/27/2022 17:24:54 - INFO - __main__ -     Num Epochs = 5\n",
      "12/27/2022 17:24:55 - INFO - __main__ -     Batch size per GPU = 64\n",
      "12/27/2022 17:24:55 - INFO - __main__ -     Total train batch size = 64\n",
      "12/27/2022 17:24:55 - INFO - __main__ -     Total optimization steps = 1048576\n",
      "  0%|                                                                                         | 0/1024 [00:00<?, ?it/s]C:\\Users\\2022010560\\AppData\\Local\\Temp\\ipykernel_17688\\1693899490.py:357: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets_x = torch.tensor(targets_x, dtype=torch.long)\n",
      "Train Epoch: 1/   5. Iter: 1024/1024. LR: 0.0300. Data: 0.155s. Batch: 6.474s. Loss: 1.2311. Loss_x: 1.1834. Loss_u: 0.\n",
      "Test Iter:  157/ 157. Data: 0.032s. Batch: 0.204s. Loss: 1.8727. top1: 28.60. top5: 82.21. : 100%|█| 157/157 [00:32<00:\n",
      "12/27/2022 19:15:56 - INFO - __main__ -   top-1 acc: 28.60\n",
      "12/27/2022 19:15:56 - INFO - __main__ -   top-5 acc: 82.21\n",
      "12/27/2022 19:15:56 - INFO - __main__ -   Best top-1 acc: 28.60\n",
      "12/27/2022 19:15:56 - INFO - __main__ -   Mean top-1 acc: 28.60\n",
      "\n",
      "Train Epoch: 2/   5. Iter: 1024/1024. LR: 0.0300. Data: 0.181s. Batch: 6.697s. Loss: 0.7987. Loss_x: 0.6502. Loss_u: 0.\n",
      "Test Iter:  157/ 157. Data: 0.030s. Batch: 0.183s. Loss: 1.0279. top1: 64.05. top5: 95.96. : 100%|█| 157/157 [00:28<00:\n",
      "12/27/2022 21:10:11 - INFO - __main__ -   top-1 acc: 64.05\n",
      "12/27/2022 21:10:11 - INFO - __main__ -   top-5 acc: 95.96\n",
      "12/27/2022 21:10:11 - INFO - __main__ -   Best top-1 acc: 64.05\n",
      "12/27/2022 21:10:11 - INFO - __main__ -   Mean top-1 acc: 46.33\n",
      "\n",
      "Train Epoch: 3/   5. Iter: 1024/1024. LR: 0.0300. Data: 0.182s. Batch: 8.155s. Loss: 0.6123. Loss_x: 0.3738. Loss_u: 0.\n",
      "Test Iter:  157/ 157. Data: 0.034s. Batch: 0.201s. Loss: 0.7369. top1: 74.68. top5: 98.49. : 100%|█| 157/157 [00:31<00:\n",
      "12/27/2022 23:29:25 - INFO - __main__ -   top-1 acc: 74.68\n",
      "12/27/2022 23:29:25 - INFO - __main__ -   top-5 acc: 98.49\n",
      "12/27/2022 23:29:25 - INFO - __main__ -   Best top-1 acc: 74.68\n",
      "12/27/2022 23:29:25 - INFO - __main__ -   Mean top-1 acc: 55.78\n",
      "\n",
      "Train Epoch: 4/   5. Iter:   19/1024. LR: 0.0300. Data: 1.675s. Batch: 8.394s. Loss: 0.6120. Loss_x: 0.3312. Loss_u: 0."
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset.cifar import DATASET_GETTERS\n",
    "from utils import AverageMeter, accuracy\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "best_acc = 0\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint, filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint,\n",
    "                                               'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer,\n",
    "                                    num_warmup_steps,\n",
    "                                    num_training_steps,\n",
    "                                    num_cycles=7./16.,\n",
    "                                    last_epoch=-1):\n",
    "    def _lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        no_progress = float(current_step - num_warmup_steps) / \\\n",
    "            float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0., math.cos(math.pi * num_cycles * no_progress))\n",
    "\n",
    "    return LambdaLR(optimizer, _lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "def interleave(x, size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([-1, size] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n",
    "\n",
    "\n",
    "def de_interleave(x, size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([size, -1] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch FixMatch Training')\n",
    "    parser.add_argument('--gpu-id', default='0', type=int,\n",
    "                        help='id(s) for CUDA_VISIBLE_DEVICES')\n",
    "    parser.add_argument('--num-workers', type=int, default=4,\n",
    "                        help='number of workers')\n",
    "    parser.add_argument('--dataset', default='cifar10', type=str,\n",
    "                        choices=['cifar10', 'cifar100'],\n",
    "                        help='dataset name')\n",
    "    parser.add_argument('--num-labeled', type=int, default=4000,\n",
    "                        help='number of labeled data')\n",
    "    parser.add_argument(\"--expand-labels\", action=\"store_true\",\n",
    "                        help=\"expand labels to fit eval steps\")\n",
    "    parser.add_argument('--arch', default='wideresnet', type=str,\n",
    "                        choices=['wideresnet', 'resnext'],\n",
    "                        help='dataset name')\n",
    "    parser.add_argument('--total-steps', default=2**20, type=int,\n",
    "                        help='number of total steps to run')\n",
    "    parser.add_argument('--eval-step', default=1024, type=int,\n",
    "                        help='number of eval steps to run')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int,\n",
    "                        help='manual epoch number (useful on restarts)')\n",
    "    parser.add_argument('--batch-size', default=64, type=int,\n",
    "                        help='train batchsize')\n",
    "    parser.add_argument('--lr', '--learning-rate', default=0.03, type=float,\n",
    "                        help='initial learning rate')\n",
    "    parser.add_argument('--warmup', default=0, type=float,\n",
    "                        help='warmup epochs (unlabeled data based)')\n",
    "    parser.add_argument('--wdecay', default=5e-4, type=float,\n",
    "                        help='weight decay')\n",
    "    parser.add_argument('--nesterov', action='store_true', default=True,\n",
    "                        help='use nesterov momentum')\n",
    "    parser.add_argument('--use-ema', action='store_true', default=True,\n",
    "                        help='use EMA model')\n",
    "    parser.add_argument('--ema-decay', default=0.999, type=float,\n",
    "                        help='EMA decay rate')\n",
    "    parser.add_argument('--mu', default=7, type=int,\n",
    "                        help='coefficient of unlabeled batch size')\n",
    "    parser.add_argument('--lambda-u', default=1, type=float,\n",
    "                        help='coefficient of unlabeled loss')\n",
    "    parser.add_argument('--T', default=1, type=float,\n",
    "                        help='pseudo label temperature')\n",
    "    parser.add_argument('--threshold', default=0.95, type=float,\n",
    "                        help='pseudo label threshold')\n",
    "    parser.add_argument('--out', default='result',\n",
    "                        help='directory to output the result')\n",
    "    parser.add_argument('--resume', default='', type=str,\n",
    "                        help='path to latest checkpoint (default: none)')\n",
    "    parser.add_argument('--seed', default=None, type=int,\n",
    "                        help=\"random seed\")\n",
    "    parser.add_argument(\"--amp\", action=\"store_true\",\n",
    "                        help=\"use 16-bit (mixed) precision through NVIDIA apex AMP\")\n",
    "    parser.add_argument(\"--opt_level\", type=str, default=\"O1\",\n",
    "                        help=\"apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "                        \"See details at https://nvidia.github.io/apex/amp.html\")\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
    "                        help=\"For distributed training: local_rank\")\n",
    "    parser.add_argument('--no-progress', action='store_true',\n",
    "                        help=\"don't use progress bar\")\n",
    "\n",
    "    args, unknown = parser.parse_known_args() \n",
    "    global best_acc\n",
    "\n",
    "    def create_model(args):\n",
    "        if args.arch == 'wideresnet':\n",
    "            import models.wideresnet as models\n",
    "            model = models.build_wideresnet(depth=args.model_depth,\n",
    "                                            widen_factor=args.model_width,\n",
    "                                            dropout=0,\n",
    "                                            num_classes=args.num_classes)\n",
    "        elif args.arch == 'resnext':\n",
    "            import models.resnext as models\n",
    "            model = models.build_resnext(cardinality=args.model_cardinality,\n",
    "                                         depth=args.model_depth,\n",
    "                                         width=args.model_width,\n",
    "                                         num_classes=args.num_classes)\n",
    "        logger.info(\"Total params: {:.2f}M\".format(\n",
    "            sum(p.numel() for p in model.parameters())/1e6))\n",
    "        return model\n",
    "\n",
    "    if args.local_rank == -1:\n",
    "        device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')#device = torch.device('cuda', args.gpu_id)\n",
    "        args.world_size = 1\n",
    "        args.n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')#device = torch.device('cuda', args.local_rank)\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "        args.world_size = torch.distributed.get_world_size()\n",
    "        args.n_gpu = 1\n",
    "\n",
    "    args.device = device\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "\n",
    "    logger.warning(\n",
    "        f\"Process rank: {args.local_rank}, \"\n",
    "        f\"device: {args.device}, \"\n",
    "        f\"n_gpu: {args.n_gpu}, \"\n",
    "        f\"distributed training: {bool(args.local_rank != -1)}, \"\n",
    "        f\"16-bits training: {args.amp}\",)\n",
    "\n",
    "    logger.info(dict(args._get_kwargs()))\n",
    "\n",
    "    if args.seed is not None:\n",
    "        set_seed(args)\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        os.makedirs(args.out, exist_ok=True)\n",
    "        args.writer = SummaryWriter(args.out)\n",
    "\n",
    "    if args.dataset == 'cifar10':\n",
    "        args.num_classes = 10\n",
    "        if args.arch == 'wideresnet':\n",
    "            args.model_depth = 28\n",
    "            args.model_width = 2\n",
    "        elif args.arch == 'resnext':\n",
    "            args.model_cardinality = 4\n",
    "            args.model_depth = 28\n",
    "            args.model_width = 4\n",
    "\n",
    "    elif args.dataset == 'cifar100':\n",
    "        args.num_classes = 100\n",
    "        if args.arch == 'wideresnet':\n",
    "            args.model_depth = 28\n",
    "            args.model_width = 8\n",
    "        elif args.arch == 'resnext':\n",
    "            args.model_cardinality = 8\n",
    "            args.model_depth = 29\n",
    "            args.model_width = 64\n",
    "\n",
    "    if args.local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    labeled_dataset, unlabeled_dataset, test_dataset = DATASET_GETTERS[args.dataset](\n",
    "        args, './data')\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    train_sampler = RandomSampler if args.local_rank == -1 else DistributedSampler\n",
    "\n",
    "    labeled_trainloader = DataLoader(\n",
    "        labeled_dataset,\n",
    "        sampler=train_sampler(labeled_dataset),\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=True)\n",
    "\n",
    "    unlabeled_trainloader = DataLoader(\n",
    "        unlabeled_dataset,\n",
    "        sampler=train_sampler(unlabeled_dataset),\n",
    "        batch_size=args.batch_size*args.mu,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=True)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        sampler=SequentialSampler(test_dataset),\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers)\n",
    "\n",
    "    if args.local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    model = create_model(args)\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    model.to(args.device)\n",
    "\n",
    "    no_decay = ['bias', 'bn']\n",
    "    grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay': args.wdecay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = optim.SGD(grouped_parameters, lr=args.lr,\n",
    "                          momentum=0.9, nesterov=args.nesterov)\n",
    "\n",
    "    args.epochs = 5#math.ceil(args.total_steps / args.eval_step)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, args.warmup, args.total_steps)\n",
    "\n",
    "    if args.use_ema:\n",
    "        from models.ema import ModelEMA\n",
    "        ema_model = ModelEMA(args, model, args.ema_decay)\n",
    "\n",
    "    args.start_epoch = 0\n",
    "\n",
    "    if args.resume:\n",
    "        logger.info(\"==> Resuming from checkpoint..\")\n",
    "        assert os.path.isfile(\n",
    "            args.resume), \"Error: no checkpoint directory found!\"\n",
    "        args.out = os.path.dirname(args.resume)\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        best_acc = checkpoint['best_acc']\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        if args.use_ema:\n",
    "            ema_model.ema.load_state_dict(checkpoint['ema_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "    if args.amp:\n",
    "        from apex import amp\n",
    "        model, optimizer = amp.initialize(\n",
    "            model, optimizer, opt_level=args.opt_level)\n",
    "\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank],\n",
    "            output_device=args.local_rank, find_unused_parameters=True)\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Task = {args.dataset}@{args.num_labeled}\")\n",
    "    logger.info(f\"  Num Epochs = {args.epochs}\")\n",
    "    logger.info(f\"  Batch size per GPU = {args.batch_size}\")\n",
    "    logger.info(\n",
    "        f\"  Total train batch size = {args.batch_size*args.world_size}\")\n",
    "    logger.info(f\"  Total optimization steps = {args.total_steps}\")\n",
    "\n",
    "    model.zero_grad()\n",
    "    train(args, labeled_trainloader, unlabeled_trainloader, test_loader,\n",
    "          model, optimizer, ema_model, scheduler)\n",
    "\n",
    "\n",
    "def train(args, labeled_trainloader, unlabeled_trainloader, test_loader,\n",
    "          model, optimizer, ema_model, scheduler):\n",
    "    if args.amp:\n",
    "        from apex import amp\n",
    "    global best_acc\n",
    "    test_accs = []\n",
    "    end = time.time()\n",
    "\n",
    "    if args.world_size > 1:\n",
    "        labeled_epoch = 0\n",
    "        unlabeled_epoch = 0\n",
    "        labeled_trainloader.sampler.set_epoch(labeled_epoch)\n",
    "        unlabeled_trainloader.sampler.set_epoch(unlabeled_epoch)\n",
    "\n",
    "    labeled_iter = iter(labeled_trainloader)\n",
    "    unlabeled_iter = iter(unlabeled_trainloader)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        losses_x = AverageMeter()\n",
    "        losses_u = AverageMeter()\n",
    "        mask_probs = AverageMeter()\n",
    "        if not args.no_progress:\n",
    "            p_bar = tqdm(range(args.eval_step),\n",
    "                         disable=args.local_rank not in [-1, 0])\n",
    "        for batch_idx in range(args.eval_step):\n",
    "            try:\n",
    "                inputs_x, targets_x = labeled_iter.next()\n",
    "            except:\n",
    "                if args.world_size > 1:\n",
    "                    labeled_epoch += 1\n",
    "                    labeled_trainloader.sampler.set_epoch(labeled_epoch)\n",
    "                labeled_iter = iter(labeled_trainloader)\n",
    "                inputs_x, targets_x = labeled_iter.next()\n",
    "\n",
    "            try:\n",
    "                (inputs_u_w, inputs_u_s), _ = unlabeled_iter.next()\n",
    "            except:\n",
    "                if args.world_size > 1:\n",
    "                    unlabeled_epoch += 1\n",
    "                    unlabeled_trainloader.sampler.set_epoch(unlabeled_epoch)\n",
    "                unlabeled_iter = iter(unlabeled_trainloader)\n",
    "                (inputs_u_w, inputs_u_s), _ = unlabeled_iter.next()\n",
    "\n",
    "            data_time.update(time.time() - end)\n",
    "            batch_size = inputs_x.shape[0]\n",
    "            inputs = interleave(\n",
    "                torch.cat((inputs_x, inputs_u_w, inputs_u_s)), 2*args.mu+1).to(args.device)\n",
    "            targets_x = targets_x.to(args.device)\n",
    "            logits = model(inputs)\n",
    "            logits = de_interleave(logits, 2*args.mu+1)\n",
    "            logits_x = logits[:batch_size]\n",
    "            logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
    "            del logits\n",
    "            targets_x = torch.tensor(targets_x, dtype=torch.long)\n",
    "            Lx = F.cross_entropy(logits_x, targets_x, reduction='mean')\n",
    "\n",
    "            pseudo_label = torch.softmax(logits_u_w.detach()/args.T, dim=-1)\n",
    "            max_probs, targets_u = torch.max(pseudo_label, dim=-1)\n",
    "            mask = max_probs.ge(args.threshold).float()\n",
    "\n",
    "            Lu = (F.cross_entropy(logits_u_s, targets_u,\n",
    "                                  reduction='none') * mask).mean()\n",
    "\n",
    "            loss = Lx + args.lambda_u * Lu\n",
    "\n",
    "            if args.amp:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            losses.update(loss.item())\n",
    "            losses_x.update(Lx.item())\n",
    "            losses_u.update(Lu.item())\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if args.use_ema:\n",
    "                ema_model.update(model)\n",
    "            model.zero_grad()\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            mask_probs.update(mask.mean().item())\n",
    "            if not args.no_progress:\n",
    "                p_bar.set_description(\"Train Epoch: {epoch}/{epochs:4}. Iter: {batch:4}/{iter:4}. LR: {lr:.4f}. Data: {data:.3f}s. Batch: {bt:.3f}s. Loss: {loss:.4f}. Loss_x: {loss_x:.4f}. Loss_u: {loss_u:.4f}. Mask: {mask:.2f}. \".format(\n",
    "                    epoch=epoch + 1,\n",
    "                    epochs=args.epochs,\n",
    "                    batch=batch_idx + 1,\n",
    "                    iter=args.eval_step,\n",
    "                    lr=scheduler.get_last_lr()[0],\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    loss_x=losses_x.avg,\n",
    "                    loss_u=losses_u.avg,\n",
    "                    mask=mask_probs.avg))\n",
    "                p_bar.update()\n",
    "\n",
    "        if not args.no_progress:\n",
    "            p_bar.close()\n",
    "\n",
    "        if args.use_ema:\n",
    "            test_model = ema_model.ema\n",
    "        else:\n",
    "            test_model = model\n",
    "\n",
    "        if args.local_rank in [-1, 0]:\n",
    "            test_loss, test_acc = test(args, test_loader, test_model, epoch)\n",
    "\n",
    "            args.writer.add_scalar('train/1.train_loss', losses.avg, epoch)\n",
    "            args.writer.add_scalar('train/2.train_loss_x', losses_x.avg, epoch)\n",
    "            args.writer.add_scalar('train/3.train_loss_u', losses_u.avg, epoch)\n",
    "            args.writer.add_scalar('train/4.mask', mask_probs.avg, epoch)\n",
    "            args.writer.add_scalar('test/1.test_acc', test_acc, epoch)\n",
    "            args.writer.add_scalar('test/2.test_loss', test_loss, epoch)\n",
    "\n",
    "            is_best = test_acc > best_acc\n",
    "            best_acc = max(test_acc, best_acc)\n",
    "\n",
    "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "            if args.use_ema:\n",
    "                ema_to_save = ema_model.ema.module if hasattr(\n",
    "                    ema_model.ema, \"module\") else ema_model.ema\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model_to_save.state_dict(),\n",
    "                'ema_state_dict': ema_to_save.state_dict() if args.use_ema else None,\n",
    "                'acc': test_acc,\n",
    "                'best_acc': best_acc,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "            }, is_best, args.out)\n",
    "\n",
    "            test_accs.append(test_acc)\n",
    "            logger.info('Best top-1 acc: {:.2f}'.format(best_acc))\n",
    "            logger.info('Mean top-1 acc: {:.2f}\\n'.format(\n",
    "                np.mean(test_accs[-20:])))\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        args.writer.close()\n",
    "\n",
    "\n",
    "def test(args, test_loader, model, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    if not args.no_progress:\n",
    "        test_loader = tqdm(test_loader,\n",
    "                           disable=args.local_rank not in [-1, 0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            data_time.update(time.time() - end)\n",
    "            model.eval()\n",
    "\n",
    "            inputs = inputs.to(args.device)\n",
    "            targets = targets.to(args.device)\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "\n",
    "            prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.shape[0])\n",
    "            top1.update(prec1.item(), inputs.shape[0])\n",
    "            top5.update(prec5.item(), inputs.shape[0])\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if not args.no_progress:\n",
    "                test_loader.set_description(\"Test Iter: {batch:4}/{iter:4}. Data: {data:.3f}s. Batch: {bt:.3f}s. Loss: {loss:.4f}. top1: {top1:.2f}. top5: {top5:.2f}. \".format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    iter=len(test_loader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                ))\n",
    "        if not args.no_progress:\n",
    "            test_loader.close()\n",
    "\n",
    "    logger.info(\"top-1 acc: {:.2f}\".format(top1.avg))\n",
    "    logger.info(\"top-5 acc: {:.2f}\".format(top5.avg))\n",
    "    return losses.avg, top1.avg\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23768da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LX=F.cross"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
